[
["index.html", "BIO 4022. Manipulación de datos e investigación reproducible en R Requerimientos 0.1 Antes de comenzar 0.2 Descripción del curso 0.3 Objetivos del curso 0.4 Contenidos 0.5 Metodología 0.6 Evaluación 0.7 Libros de consulta 0.8 Bibliografía", " BIO 4022. Manipulación de datos e investigación reproducible en R Derek Corcoran 2018-09-07 Requerimientos Para comenzar el trabajo se necesita la última versión de R y RStudio (R Core Team 2018).También se requiere de los paquetes pacman, rmarkdown, tidyverse y tinytex. Si no se ha usado R o RStudio anteriormente, el siguiente video muestra cómo instalar ambos programas y los paquetes necesarios para este curso en el siguiente link. El código para la instalación de esos paquetes es el siguiente: install.packages(&quot;pacman&quot;, &quot;rmarkdown&quot;, &quot;tidyverse&quot;, &quot;tinytex&quot;) En caso de necesitar ayuda para la instalación, contactarse con el instructor del curso. 0.1 Antes de comenzar Si nunca se ha trabajado con R antes de este curso, una buena herramienta es provista por el paquete Swirl (Kross et al. 2017). Para comenzar la práctica, realizar los primeros 7 modulos del programa R Programming: The basics of programming in R que incluye: Basic Building Blocks Workspace and Files Sequences of Numbers Vectors Missing Values Subsetting Vectors Matrices and Data Frames El siguiente link muestra un video explicativo de cómo usar el paquete swirl Video 0.2 Descripción del curso Este curso está enfocado en entregar principios básicos de investigación reproducible en R, con énfasis en la recopilación y/o lectura de datos de forma reproducible y automatizada. Para esto se trabajará con bases de datos complejas, las cuales deberán ser transformadas y organizadas para optimizar su análisis. Se generarán documentos reproducibles integrando en un documento: código, bibliografía, exploración y análisis de datos. Se culminará el curso con la generación de un manuscrito, una presentación y/o un documento interactivo reproducible. 0.3 Objetivos del curso Conocer y entender el concepto de investigación reproducible como una forma y filosofía de trabajo que permite que las investigaciones sean más ordenadas y replicables, desde la toma de datos hasta la escritura de resultados. Conocer y aplicar el concepto de pipeline, el cual permite generar una modularidad desde la toma de datos hasta la escritura de resultados, donde la corrección independiente de un paso tiene un efecto cascada sobre el resultado final. Aprender buenas prácticas de recolección y estandarización de bases de datos, con la finalidad de optimizar el análisis de datos y la revisión de éstas por pares. Realizar análisis críticos de la naturaleza de los datos al realizar análisis exploratorios, que permitirán determinar la mejor forma de comprobar hipótesis asociadas a estas bases de datos. 0.4 Contenidos Capítulo 1 Tidy Data: En este capítulo se aprenderá a cómo optimizar una de base de datos, sobre la limpieza y transformación de bases de datos, qué es una base de datos tidy y cómo manipular estas bases de datos con el paquete dplyr (Wickham et al. 2018). Capítulo 2 Investigación reproducible: En este capítulo se trabajará en la confección de un documento que combine códigos de R y texto para generar documentos reproducibles utilizando el paquete rmarkdown (Allaire et al. 2018). Además, se verá cómo al usar RStudio se pueden guardar los proyectos en un repositorio de github. Capítulo 3 El tidyverso y el concepto de pipeline:En este capítulo se aprenderá sobre la limpieza de datos complejos. Capítulo 4 Visualización de datos visualizar datos vs. visualizar modelos. Insertar gráficos con leyenda en un documento Rmd Capítulo 5 Modelos en R Aprender a generar modelos en R, desde ANOVA a GLM. Capítulo 6 Loops. Generación de funciones propias en R y loops Escritura de manuscritos en R, transformación de documentos Rmd en un manuscrito Presentaciones en R y generar documentos interactivos. Transformación de datos en una presentación o en una Shiny app. Realizar una presentación o aplicación en R. 0.5 Metodología Todas las clases estarán divididas en dos partes: I. Clases expositivas de principios y herramientas, donde se presentarán los principios de investigación reproducible y tidy data, junto con las herramientas actuales más utilizadas, y II. Clases prácticas donde cada estudiante trabajará con datos propios para desarrollar un documento reproducible. Los estudiantes que no cuenten con datos propios podrán acceder a sets de datos para su trabajo o podrán simularlos, dependiendo del caso. Además, se deberán generar informes y presentaciones siguiendo los principios de investigación reproducible, en base al trabajo con sus datos. Se realizará un informe final, en el cual se espera un trabajo que compile los conociminetos adquiridos durante el curso. 0.6 Evaluación Evaluación 1: Informe exploratorio de base de datos 25% Evaluación 2: Presentación 25% Evaluación 3: Informe final 50% 0.7 Libros de consulta Los principios de este curso están explicados en los siguientes libros gratuitos. Gandrud, Christopher. Reproducible Research with R and R Studio. CRC Press, 2013. Available for free in the following link Stodden, Victoria, Friedrich Leisch, and Roger D. Peng, eds. Implementing reproducible research. CRC Press, 2014. Available for free in the following link 0.8 Bibliografía Referencias "],
["tidydata.html", "Capítulo 1 Tidy Data y manipulación de datos 1.1 Paquetes necesarios para este capítulo 1.2 Tidy data 1.3 dplyr 1.4 left join", " Capítulo 1 Tidy Data y manipulación de datos 1.1 Paquetes necesarios para este capítulo Para este capitulo necesitas tener instalado el paquete tidyverse En este capítulo se explicará qué es una base de datos tidy (Wickham and others 2014) y se aprenderá a usar funciones del paquete dplyr (Wickham et al. 2018) para manipular datos. Dado que este libro es un apoyo para el curso BIO4022, esta clase del curso puede también ser seguida en este link. El video de la clase se encuentra disponible en este link. 1.2 Tidy data Una base de datos tidy es una base de datos en la cuál (modificado de (Leek 2015)): Cada vararible que se medida debe estar en una columna. Cada observación distinta de esa variable debe estar en una fila diferente. En general, la forma en que representaríamos una base de datos tidy en R es usando un data frame. 1.3 dplyr El paquete dplyr es definido por sus autores como una gramática para la manipulación de datos. De este modo sus funciones son conocidas como verbos. Un resumen útil de muchas de estas funciones se encuentra en este link. Este paquete tiene un gran número de verbos y sería difícil ver todos en una clase, en este capítulo nos enfocaremos en sus funciones más utilizadas, las cuales son: group_by (agrupa datos) summarize (resume datos agrupados) mutate (genera variables nuevas) %&gt;% (pipeline) filter (encuentra filas con ciertas condiciones) select junto a starts_with, ends_with o contains 1.3.1 summarize La función summarize toma los datos de un data frame y los resume. Para usar esta función, el primer argumento que tomaríamos sería un data frame, se continúa del nombre que queremos darle a una variable resumen, seguida del signo = y luego la fórmula a aplicar a una o mas columnas. COmo un ejemplo se utilizará la base de datos iris (Anderson 1935) que viene en R y de las cual podemos ver parte de sus datos en la tabla ?? Si quisieramos resumir esa tabla y generar un par de variables que fueran la media y la desviación estándar del largo del pétalo, lo haríamos con el siguiente código: library(tidyverse) Summary.Petal &lt;- summarize(iris, Mean.Petal.Length = mean(Petal.Length), SD.Petal.Length = sd(Petal.Length)) El resultado se puedde ver en la tabla ??, en el cuál se obtienen los promedios y desviaciones estándar de los largos de los pétalos. Es importante notar que al usar summarize, todas las otras variables desapareceran de la tabla. 1.3.2 group_by La función group_by por si sola no genera cambios visibles en las bases de datos. Sin embargo, al ser utilizada en conjunto con summarize permite resumir una variable agrupada (usualmente) basada en una o más variables categóricas. Se puede ver que para el ejemplo con el caso de las plantas del género Iris, el resumen que se obtiene en el caso de la tabla ?? no es tan útil considerando que tenemos tres especies presentes. Si se quiere ver el promedio del largo del pétalo por especie, se debe ocupar la función group_by de la siguiente forma: BySpecies &lt;- group_by(iris, Species) Summary.Byspecies &lt;- summarize(BySpecies, Mean.Petal.Length = mean(Petal.Length), SD.Petal.Length = sd(Petal.Length)) Esto dá como resultado la tabla ??, con la cuál se puede ver que Iris setosa tiene pétalos mucho más cortos que las otras dos especies del mismo género. 1.3.2.1 group_by en más de una variable Se puede usar la función group_by en más de una variable, y esto generaría un resumen anidado. Como ejemplo se usará la base de datos mtcars presente en R (Henderson and Velleman 1981). Esta base de datos presenta una variable llamada mpg (miles per gallon) y una medida de eficiencia de combustible. Se resumirá la información en base a la variable am (que se refiere al tipo de transmisión, donde 0 es automático y 1 es manual) y al número de cilindros del motor. Para eso se utilizará el siguiente código: Grouped &lt;- group_by(mtcars, cyl, am) Eficiencia &lt;- summarize(Grouped, Eficiencia = mean(mpg)) Como puede verse en la tabla ??, en todos los casos los autos con cambios manuales tienen mejor eficiencia de combustible. Se podría probar el cambiar el orden de las variables con las cuales agrupar y observar los distintos resultados que se pueden obtener. 1.3.3 mutate Esta función tiene como objetivo crear variables nuevas basadas en otras variables. Es muy facil de usar, como argumento se usa el nombre de la variable nueva que se quiere crear y se realiza una operación con variables que ya estan ahí. Por ejemplo, si se continúa el trabajo con la base de datos Iris, al crear una nueva variable que sea la razón entre el largo del pétalo y el del sépalo, resulta lo siguiente: DF &lt;- mutate(iris, Petal.Sepal.Ratio = Petal.Length/Sepal.Length) El resultado de esta operación es la tabla ??. Siempre la variable que se acaba de crear aparecerá al final del data frame. 1.3.4 Pipeline (%&gt;%) El pipeline es un simbolo operatorio %&gt;% que sirve para realizar varias operaciones de forma secuencial sin recurrir a parentesis anidados o a sobrescribir muúltiples bases de datos. Para ver como funciona esto como un vector, supongamos que se tiene una variable a la cual se quiere primero obtener su logaritmo, luego su raíz cuadrada y finalmente su promedio con dos cifras significativas. Para realizar esto se debe seguir lo siguiente: x &lt;- c(1, 4, 6, 8) y &lt;- round(mean(sqrt(log(x))), 2) Si se utiliza pipeline, el código sería mucho más ordenado. En ese caso, se partiría por el objeto a procesar y luego cada una de las funciones con sus argumentos si es necesario: x &lt;- c(1, 4, 6, 8) y &lt;- x %&gt;% log() %&gt;% sqrt() %&gt;% mean() %&gt;% round(2) ## [1] 0.99 El código con pipeline es mucho más fácil de interpretar a primera vista ya que se lee de izquierda a derecha y no de adentro hacia afuera. EL uso de pipeli se hace aun más importante cuando se usa con un Data frame, como se ve en el siguiente ejemplo: 1.3.4.1 El pipeline en data frames POr ejemplo se quiere resumir la variable recien creada de la razón entre el sépalo y el petalo. Para hacer esto, si se partiera desde la base de datos original, tomaría varias líneas de código y la creación de múltiples bases de datos intermedias DF &lt;- mutate(iris, Petal.Sepal.Ratio = Petal.Length/Sepal.Length) BySpecies &lt;- group_by(DF, Species) Summary.Byspecies &lt;- summarize(BySpecies, MEAN = mean(Petal.Sepal.Ratio), SD = sd(Petal.Sepal.Ratio)) Otra opción es usar paréntesis anidados, lo que se traduce en el siguiente código: Summary.Byspecies &lt;- summarize(group_by(mutate(iris, Petal.Sepal.Ratio = Petal.Length/Sepal.Length), Species), MEAN = mean(Petal.Sepal.Ratio), SD = sd(Petal.Sepal.Ratio)) Esto se simplifica mucho más al usar el pipeline, lo cual permite partir en un Data Frame y luego usar el pipeline. Esto permite obtener el mismo resultado que en las operaciones anteriores con el siguiente código: Summary.Byspecies &lt;- iris %&gt;% mutate(Petal.Sepal.Ratio = Petal.Length/Sepal.Length) %&gt;% group_by(Species) %&gt;% summarize(MEAN = mean(Petal.Sepal.Ratio), SD = sd(Petal.Sepal.Ratio)) Estos tres códigos son correctos (tabla ??), pero definitivamente el uso del pipeline da el código más conciso y fácil de interpretar sin pasos intermedios. 1.3.5 filter Esta función permite seleccionar filas que cumplen con ciertas condiciones, como tener un valor mayor a un umbral o pertenecer a cierta clase Los símbolos más típicos a usar en este caso son los que se ven en la tabla ??. Por ejemplo si se quiere estudiar las características florales de las plantas del género Iris, pero no tomar en cuenta a la especie Iris versicolor se deberá usar el siguiente código: data(&quot;iris&quot;) DF &lt;- iris %&gt;% filter(Species != &quot;versicolor&quot;) %&gt;% group_by(Species) %&gt;% summarise_all(mean) De esta forma se obtiene como resultado la tabla ??. En este caso se introduce la función summarize_all de summarize, la cual aplica la función que se le da como argumento a todas las variables de la base de datos. Por otro lado si se quiere estudiar cuántas plantas de cada especie tienen un largo de pétalo mayor a 4 y un largo de sépalo mayor a 5 se deberá usar el siguiente código: DF &lt;- iris %&gt;% filter(Petal.Length &gt;= 4 &amp; Sepal.Length &gt;= 5) %&gt;% group_by(Species) %&gt;% summarise(N = n()) En la tabla tabla ?? se ve que con este filtro desaparecen de la base de datos todas las plantas de Iris setosa y que todas menos una planta de Iris virginica tienen ambas características. 1.3.6 select Esta función permite seleccionar las variables a utilizar dado que en muchos casos nos encontraremos con bases de datos con demasiadas variables y por lo tanto, se querrá reducirlas para solo trabajar en una tabla con las variables necesarias. Con select hay varias formas de trabajar, por un lado se puede escribir las variables que se utilizarán, o restar las que no. En ese sentido estos cuatro códigos dan exactamente el mismo resultado. Esto se puede ver en la tabla ?? iris %&gt;% group_by(Species) %&gt;% select(Petal.Length, Petal.Width) %&gt;% summarize_all(mean) iris %&gt;% group_by(Species) %&gt;% select(-Sepal.Length, -Sepal.Width) %&gt;% summarize_all(mean) iris %&gt;% group_by(Species) %&gt;% select(contains(&quot;Petal&quot;)) %&gt;% summarize_all(mean) iris %&gt;% group_by(Species) %&gt;% select(-contains(&quot;Sepal&quot;)) %&gt;% summarize_all(mean) 1.3.7 Joins Los ejemplos a continuación se basan en el código generado por Garrick Aden-Buie en su repositorio de animaciones de verbos del tidyverse (Aden-Buie 2018). El paquete dplyr, tiene una serie de funciones de apellido join: anti_join, full_join, inner_join, left_join, right_join y semi_join, en general no son tan fáciles de entender a primera vista, por lo que se trabajará con dos tablas muy simples (Tabla 1.1), las cuales tienen dos columnas cada una Tabla 1.1: Dos tablas para unir. id x 1 x1 2 x2 3 x3 id y 1 y1 2 y2 4 y4 1.4 left join Como vemos en la figura 1.1 Figura 1.1: animación de left join entre las tablas x e y, cache = TRUE Entonces 1.4.1 Ejercicios 1.4.1.1 Ejercicio 1 Usando la base de datos storms del paquete dplyr, calcular la velocidad promedio y diámetro promedio (hu_diameter) de las tormentas que han sido declaradas huracanes para cada año. 1.4.1.2 Ejercicio 2 La base de datos mpg del paquete ggplot2 tiene datos de eficiencia vehicular en millas por galón en ciudad (cty) en varios vehículos. Obtener los datos de vehículos del año 2004 en adelante que sean compactos y transformar la eficiencia Km/litro (1 milla = 1.609 km; 1 galón = 3.78541 litros) Las soluciones a estos ejercicios se encuentran en el capítulo 8 Referencias "],
["reproducible.html", "Capítulo 2 Investigación reproducible 2.1 Paquetes necesarios para este capítulo 2.2 Investigación reproducible 2.3 Guardando nuestro proyecto en github 2.4 Reproducibilidad en R subtitulo 1", " Capítulo 2 Investigación reproducible 2.1 Paquetes necesarios para este capítulo Para este capítulo se necesita tener instalado los paquetes rmarkdown, knitr y stargazer En este capítulo se explicará qué es investigación reproducible, cómo aplicarla usando github más los paquetes rmarkdown (Allaire et al. 2018) y knitr (Xie 2015). Además, se aprenderá a usar tablas usando knitr (Xie 2015) y stargazer (Hlavac 2018) Recuerda que este libro es un apoyo para el curso BIO4022, puedes seguir la clase de este curso en este link, y en cuanto el video de la clase encontrarás un link aca. 2.2 Investigación reproducible La investigación reproducible no es lo mismo que la investigación replicable. La replicabilidad implica que experimentos o estudios llevados a cabo en condiciones similares nos llevarán a conclusiones similares. La investigación reproducible implica que desde los mismos datos y/o el mismo código se generarán los mismos resultados. Figura 2.1: Continuo de reproducibilidad (extraido de Peng 2011) En la figura 2.1 vemos el continuo de reproducibilidad (Peng 2011). En este continuo tenemos el ejemplo de no reproducibilidad como una publicación sin código. Se pasa de menos a más reproducible por la publicación y el código que generó los resultados y gráficos; seguido por la publicación, el código y los datos que generan los resultados y gráficos; y por último código, datos y texto entrelazados de forma tal que al correr el código obtenemos exactamente la mismma publicación que leímos. Esto tiene muchas ventajas, incluyendo el que es más fácil aplicar exactamente los mismos métodos a otra base de datos. Basta poner la nueva base de datos en el formato que tenía el autor de la primera publicación y podremos comparar los resultados. Además en un momento en que la ciencia está basada cada vez más en bases de datos, se puede poner en el código la recolección y/o muestreo de datos. 2.3 Guardando nuestro proyecto en github 2.3.1 Que es github? Github es una suerte de dropbox o google drive pensado para la investigación reproducible, en donde cada proyecto es un repositorio. La mayoría de los investigadores que trabajan en investigación reproducible dejan todo su trabajo documentado en sus repositorios, lo cual permite interactuar con otros autores. 2.3.2 creando un proyecto de github en RStudio Para crear un proyecto en github presionamos start a project en la página inicial de nuestra cuenta, como vemos en la figura 2.2 Figura 2.2: Para empezar un projecto en github, debes presionar Start a project en tu página de inicio Luego se debe crear un nombre único, y sin cambiar nada más presiona create repository en el botón verde como vemos en la figura 2.3. Figura 2.3: Crea el nombre de tu repositorio y apreta el boton create repository Esto te llevará a una página donde aparecerá una url de tu nuevo repositorio como en la figura 2.4 Figura 2.4: El contenido del cuadro en el cual dice ssh es la url de tu repisitorio Para incorporar tu proyecto en tu repositorio, lo primero que debes hacer es generar un proyecto en RStudio. Para esto debes ir en el menú superior de Rstudio a File &gt; New Project &gt; Git como se ve en las figuras 2.5 y 2.5. Figura 2.5: Menú para crear un proyecto nuevo Figura 2.6: Seleccionar git dentro de las opciones Luego seleccionar la ubicación del proyecto nuevo y pegar el url que aparece en la figura 2.4 en el espacio que dice Repository URL:, como muestra en la figura 2.7. Figura 2.7: Pegar el url del repositorio en el cuadro de dialogo Repository URL: Cuando tu proyecto de R ya este siguiendo los cambios en github, te aparecerá una pestaña git dentro de la ventana superior derecha de tu sesión de RStudio, tal como vemos en la figura 2.8 Figura 2.8: Al incluir tu repositorio en tu sesión de Rstudio, aparecera la pestaña git en la ventana superior derecha 2.3.3 Los tres principales pasos de un repositorio Github es todo un mundo, existen muchas funciones y hay expertos en el uso de github. En este curso, nos enfocaremos en los 3 pasos principales de un repositorio: add, commit y push. Para entender bien qué significa cada uno de estos pasos, tenemos que entender que existen dos repositorios en todo momento: uno local (en tu computador) y otro remoto (en github.com). Los dos primeros pasos add y commit, solo generan cambios en tu repositorio local. Mientras que push, salva los cambios al repositorio remoto. 2.3.3.1 git add Esta función es la que agrega archivos a tu repositorio local. Solo estos archivos serán guardados en github. Github tienen un límite de tamaño de repositorio de 1 GB y de archivos de 100 MB, ya que si bien te dan repositorios ilimitados, el espacio de cada uno no lo es, en particular en cuanto a bases de datos. Para adicionar un archivo a tu repositorio tan solo debes selecionar los archivos en la pestaña git. Al hacer eso una letra A verde aparecerá en vez de los dos signos de interrogación amarillos, como vemos en la figura 2.9. En este caso solo adicionamos al repositorio el archivo Analisis.r pero no el resto. Figura 2.9: Al incluir tu repositorio en tu sesión de Rstudio, aparecera la pestaña git en la ventana superior derecha 2.3.3.2 git commit Cuando ocupas el comando commit estas guardando los cambios de los archivos que adicionaste en tu repositorio local. Para hacer esto en Rstudio, en la misma pestaña de git, debes presionar el botón commit como vemos en la figura 2.10. Figura 2.10: Para guardar los cambios en tu repositorio apretar commit en la pestaña git de la ventana superior derecha Al presionar commit, se abrirá una ventana emergente, donde deberás escribir un mensaje que describa lo que guardarás. Una vez echo eso, presiona commit nuevamente en la ventana emergente como aparece en la figura 2.11. Figura 2.11: Escribir un mensaje que recuerde los cambios que hiciste en la ventana emergente 2.3.3.3 git push Finalmente, push te permitirá guardar los cambios en tu repositorio remoto, lo cual asegura tus datos en la nube y además lo hace disponible a otros investigadores. Luego de apretar commit en la ventana emergente (figura 2.11), podemos presionar push en la flecha verde de la ventana emergente como se ve el a figura 2.12. Luego se nos pedirá nuestro nombre de usuario y contraseña, y ya podemos revisar que nuestro repositorio esta online entrando a nuestra sesión de github. Figura 2.12: Para guardar en el repositorio remoto apretar push en la ventana emergente 2.4 Reproducibilidad en R Existen varios paquetes que permiten que hagamos investigación reproducible en R, pero sin duda los más relevantes son rmarkdown y knitr. Ambos paquetes funcionan en conjunto cuando generamos un archivo Rmd (Rmarkdown), en el cual ocupamos al mismo tiempo texto, código de R y otros elementos para generar un documento word, pdf, página web, presentación y/o aplicación web (fig 2.13). Figura 2.13: El objetivo de Rmarkdown es el unir código de r con texto y datos para generar un documento reproducible 2.4.1 Creando un Rmarkdown Para crear un archivo Rmarkdown, simplemente ve a el menu File &gt; New file &gt; Rmarkdown y con eso habrás creado un nuevo archivo Rmd. Veremos algunos de los elementos más típicos de un archivo Rmarkdown. 2.4.1.1 Markdown El markdown es la parte del archivo en que simplemente escribimos texto, aunque tiene algunos detalles para el formato como generar texto en negrita, cursiva, títulos y subtitulos. Para hacer que un texto este en negrita, se debe poner entre dos asteriscos **negrita**, para que un texto aparezca en cursiva debe estar entre asteriscos *cursiva*. Otros ejemplos son los títulos de distintos niveles, los cuales se denotan con distintos números de #, así los siguientes 4 títulos o subtítulos: subtitulo 1 subtítulo 2 subtítulo 3 subtítulo 4 se vería de la siguiente manera en el código ## subtitulo 1 ### subtítulo 2 #### subtítulo 3 ##### subtítulo 4 2.4.1.2 Chunks Los chunks son una de las partes más importantes del un Rmarkdown. En estos es donde se agrega el código de R (u otros lenguajes de programación). Lo cual permíte que el producto de nuestro código no sea sólo un escrito con resultados pegados, sino que efectivamente generados en el mismo documento que nuestro escrito. La forma más fácil de agregar un chunk es apretando el botón de insert chunk en Rstudio, este boton se encuentra en la ventana superior izquierda de nuestra sesión de RStudio, tal como se muestra en la figura 2.14 Figura 2.14: Al apretar el botón insert chunk, aparecera un espacio en el cuál insertar código Al apretar este botón aparecera un espacio, ahí se puede agregar un código como el que aparece a continuación, y ver a continuación los resultados. ```{r} library(tidyverse) iris %&gt;% group_by(Species) %&gt;% summarize(Petal.Length = mean(Petal.length)) ``` ## # A tibble: 3 x 2 ## Species Petal.Length ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 2.4.1.2.1 Opciones de los chunks Existen muchas opciones para los chunks, una documentación completa podemos encontrarle en el siguiente link, pero acá mostraremos los más comunes: echo = T o F muestro o no el código, respectivamente message = T o F muestra mensajes de paquetes, respectivamente warning = T o F muestra advertencias, respectivamente eval = T o F evaluar o no el código, respectivamente cache = T o F guarda o no el resultado, respectivamente 2.4.1.3 inline code Los inline codes son útiles para agregar algún valor en el texto, como por ejemplo el valor de p o la media. Para usarlo, se debe poner un backtick (comilla simple hacia atrás), r, el código en cuestion y otro backtick como se ve a continuación `r R_código`. No podemos poner cualquier cosa en un inline code, ya que sólo puede generar vectores, lo cuál muchas veces requiere de mucha creatividad para lograr lo que queremos. Por ejemplo si se quisiera poner el promedio del largo del sépalo de la base da dato iris en un inline code pondríamos `r mean(iris$Sepal.Length)`, lo cual resultaría en 5.8433333. Como en un texto se vería extraño un número con 7 cifras significativas, querríamos usar ademas la función round, para que tenga 2 cifras significativas, para eso ponemos el siguiente inline code `r round(mean(iris$Sepal.Length),2)` que da como resultado 5.84. Esto se puede complejizar más aún si se quiere trabajar con una tabla resumen. Por ejemplo, si quisieramos listar el promedio del tamaño de sépalo usaríamos summarize de dplyr, pero esto nos daría como resultado un data.frame, el cual no aparece si intentamos hacer un inline code. Partamos por ver como se vería el código donde obtuvieramos la media del tamaño del sépalo. iris %&gt;% group_by(Species) %&gt;% summarize(Mean = mean(Sepal.Length)) El resultado de ese código lo veríamos ?? Para sacar de este data frame el vector de la media podríamos subsetearlo con el signo $. Entonces si queremos sacar como vector la columna Mean del data frame que creamos, haríamos lo siguiente `r (iris %&gt;% group_by(Species) %&gt;% summarize(Mean = mean(Sepal.Length)))$Mean`. Esto daría como resultado 5.006, 5.936, 6.588. 2.4.2 Ejercicios 2.4.2.1 Ejercicio 1 Usando la base de datos iris, crea un inline code que diga cuál es la media del largo del pétalo de la especie Iris virginica La solución a este ejercicio se encuentra en el capítulo 8 2.4.2.2 Tablas en Rmarkdown La función más típica para generar tablas en un archivo rmd es kable del paquete knitr, que en su forma más simple se incluye un dataframe como único argumento. Además de esto, podemos agregar algunos parámetros como caption, que nos permite poner un título a la tabla o row.names, que si se pone como se ve en el código (FALSE) no mostrará en la tabla los nombres de las filas, tal como se ve en la tabla ??. DF &lt;- iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) kable(DF, caption = &quot;Promedio por especie de todas las variables de la base de datos iris.&quot;, row.names = FALSE) Referencias "],
["tidyverso.html", "Capítulo 3 El Tidyverso y tidyr 3.1 Paquetes necesarios para este capítulo 3.2 El tidyverso 3.3 tidyr 3.4 Ejercicios", " Capítulo 3 El Tidyverso y tidyr 3.1 Paquetes necesarios para este capítulo Para este capítulo necesitas tener instalado el paquete tidyverse y el paquete dismo para uno de los ejercicios. En este capítulo se explicará qué es el paquete tidyverse (Wickham 2017) y cuales son sus componentes. Además veremos las funciones del paqute tidyr (Wickham and Henry 2018) con sus dos funciones gather y spread. Dado que este libro es un apoyo para el curso BIO4022, esta clase puede también ser seguida en este link. El video de la clase se encontrará disponible en este link. 3.2 El tidyverso El tidiverso se refiere al paquete tidiverse, el cual es una colección de paquetes coehrentes, que tienen una gramática, filosofía y estructura similar. Todos se basan en la idea de tidy data propuesta por Hadley Wickham (Wickham and others 2014). Los paquetes que forman parte del tidyverso son: readr (ya la estamos usando) dplyr (Clase anterior) tidyr (Hoy) ggplot2 (Próxima clase) purrr (En clase sobre loops) forcats (Para variables categóricas) stringr (Para carácteres, Palabras) 3.2.1 readr El paquete readr (Wickham, Hester, and Francois 2017) tiene como función el importar (leer) y exportar archivos. Dado que en general nosotros usaremos archivos del tipo csv, para este tipo de archivos, readr tiene la función read_csv. Para exportar un archivo ocupamos la función write_csv. Ambas funciones son 10 veces más rápidas que las versiones de r base. Para más información sobre este revisar su página oficial. 3.2.2 dplyr Este paquete sirve para modificar variables y sus detalles los vimos en el capítulo 1. Los cinco verbos principales que tiene son mutate para generar nuevas variables y que vienen de variables ya existentes, select para seleccionar variables basadas en su nombre, filter para seleccionar filas de acuerdo a si cumplen o no con condiciones en una o mas variables, summarize para resumir las variables, y arrange para reordenar las filas de acuerdo a alguna variable. Para más información sobre este paquete revisar su página oficial. 3.2.3 tidyr Con sólo dos funciones: gather y spread. El paquete tidyr (Wickham and Henry 2018) tiene como finalidad el tomar bases de datos no tidy y transformalas en tidy (datos limpios y ordenados). Para esto, gather transforma tablas anchas en largas y spread transforma tablas anchas en larga. En este capítulo explicaremos en más detalle estos dos verbos. Para más información sobre este paquete revisar su página oficial. 3.2.4 ggplot2 Una vez que una base de datos está en formato tidy, podemos usar ggplot2 (Wickham 2016) para visualizar estos datos. Los datos pueden ser categóricos, continuos e incluso espaciales en conjunto con el paquete sf. Este paquete es el más antiguo del tidyverse y por ello posee una gramática un poco diferente. Hablaremos más de este paquete en el capítulo 4. Por ahora si se quiere aprender más sobre ggplot2 se puede revisar la página oficial 3.2.5 purrr Purrr (Henry and Wickham 2018) permite formular loops de una forma más sencilla e intuitiva que los for loops. Utilizando sus funciones map, map2, walk y reduce podemos realizar loops dentro de la gramática del tidyverse. Trabajaremos en este paquete en el capítulo 6. Como siempre puedes encontras más información en su página oficial 3.2.6 forcats Trabajar con factores es una de las labores más complejas en R, es por eso que se creó el paquete forcats (Wickham 2018a). Si bien no hay un capítulo en este libro en el cuál se trabajará exclusivamente con este paquete, se utilizará al menos una función en el capítulo 4 3.2.7 stringr El modíficar variables de texto para hacer que las variables tengan sentido humano es algo muy importante, para este tipo de modificaciones se utiliza el paquete stringr (Wickham 2018b). En este capítulo, para algunos ejercicios, introduciremos algunas funcionalidades de este paquete. Para más información revisar su página oficial. 3.3 tidyr Este paquete como ya fue explicado en la sección anterior, solo posee dos funciones: gather y spread. Estas funciones sirven para pasar de tablas anchas a largas y viceversa, pero ¿qué significa que la misma información sea presentada en una tabla larga o en una tabla ancha? Tomemos por ejemplo dos tablas. En la tabla ?? vemos una tabla ancha y en la tabla ?? una tabla larga. 3.3.0.1 DATO Usualmente las tablas anchas son mejores para ser mostradas ya que se distinguen más facilmente las variables trabajadas, mientras que las tablas largas son mejores para programar y hacer análisis. 3.3.1 gather Esta función nos permite pasar de una tabla ancha a una larga. En muchos casos esto es necesario para generar una base de datos tidy, y en otras ocaciones es importante para generación de gráficos que necesitamos tal como veremos en el capítulo 4. En esta función partimos con un data frame y luego tenemos 3 argumentos: en el primero key, ponemos el nombre de la variable que va a llevar como observaciones los nombres de las columnas; luego en el argumento value, ponemos el nombre de la columna que llevará los valores de cada columna al transformarse en una columna larga; Por último hay un argumento (sin nombre), en el cual ponemos las columas que queremos que sean “alargadas”, o con un signo negativo, las que no queremos que sean parte de esta transformación. Todo esto quedará más claro en el siguiente ejemplo. 3.3.1.1 Ejemplo de los censos Supongamos que un estudiante de biología va a realizar un censo en un parque nacional por tres días y genera la siguiente tabla (el código a continuación es el que permite generar el data frame obervado en la tabla ??) df_cuentas &lt;- data.frame(dia = c(&quot;Lunes&quot;, &quot;Martes&quot;, &quot;Miercoles&quot;), Lobo = c(2, 1, 3), Liebre = c(20, 25, 30), Zorro = c(4, 4, 4)) Claramente esta base de datos no es tidy, ya que deberíamos tener una columna para la variable día, otra para especie y por último una para la abundancia de cada especie en cadad día. Antes de mostrar como realizaríamos esto con gather, veamos sus efectos para entenderlo mejor. La forma más básica de usar esta función sería el solo darle un nombre a la columna key (que tendrá el nombre de las columnas) y otro a value, que tendría el valor de las celdas. Veamos que ocurre si hacermos eso en el siguiente código y tabla ??. library(tidyverse) DF_largo &lt;- df_cuentas %&gt;% gather(key = Columnas, value = Valores) Como vemos en la tabla ??, en la columna llamada Columnas, tenemos sólo los nombres de las columnas de la tabla ??, y en la columna Valores, tenemos los valores encontrados en la tabla ??. Sin embargo, para tener las tres columnas que desearíamos tener (día, especie y abundancia), necesitamos que la variable día no participe de este “alargamiento”, para esto lo que haríamos sería los siguiente: DF_largo &lt;- df_cuentas %&gt;% gather(key = Columnas, value = Valores, -dia) Al agregar -día como tercer argumento quitamos esa variable del día en el “alargamiento”, en ese caso obtenemos la tabla ??. Ahora sólo falta arreglar los nombres. Para cambiar los nombres de las columnas que nos faltan, sólo cambiamos los valores de los argumentos key y value como se ve a continuación y en la tabla ??. DF_largo &lt;- df_cuentas %&gt;% gather(key = Especie, value = Abundancia, -dia) 3.3.2 spread spread es la función inversa a gather, esto es, toma una tabla de datos en formato ancho y la trnasforma en una base de datos de formato largo. Esta función tiene dos argumentos básicos. key que es el nombre de la variable que pasará a ser nombres de columna y value, que es el nombre de la columna con los valores que llenarán estas columnas. 3.3.2.1 Continuación ejemplo de censos Volvamos al ejemplo de los censos donde quedamos, en nuestro último ejercicio creamos el data frame DF_largo que vemos en la tabla ??. Veremos algunos ejemplos de como podemos cambiar este data frame en una tabla ancha: DF_ancho &lt;- DF_largo %&gt;% spread(key = dia, value = Abundancia) Con el código anterior generamos la ??, la cuál es distinta a la original en la tabla ??), en esta los días quedaron como nombres de columnas, y las especies pasaron a ser una variable. En la tabla ?? se ven todas las opciones de como generar una tabla ancha en base a el data frame DF_largo, pruebe opciones hasta entender la función, algunas de estas opciones darán errores. 3.4 Ejercicios 3.4.1 Ejercicio 1 Utilizando el siguiente código usando el paquete dismo bajaras la base de datos del GBIF (Global Biodiversity Information Facility) de presencias conocidas del huemul (Hippocamelus bisulcus): library(dismo) Huemul &lt;- gbif(&quot;Hippocamelus&quot;, &quot;bisulcus&quot;, down = TRUE) colnames(Huemul) Tomando la base de datos generada: Quedarse con solo las observaciones que tienen coordenadas geograficas Determinar cuantas observaciones son de observacion humana y cuantas de especimen de museo 3.4.2 Ejercicio 2 Entrar a INE ambiental y bajar la base de datos de Dimensión Aire. Generar una base de datos tidy con las siguientes 5 columnas El nombre de la localidad donde se encuntra la estación El año en que se tomo la medida El mes en que se tomo la medida La temperatura media de ese mes La media del mp25 de ese mes Humedad relativa media mensual De la base de datos anterior obterner un segundo data frame en la cual calculen para cada variable y estación la media y desviación estandar para cada mes Referencias "],
["visualizacion.html", "Capítulo 4 Visualización de datos 4.1 Paquetes necesarios para este capítulo 4.2 El esqueleto 4.3 Por que usamos aes() y + 4.4 geom_algo 4.5 Combinando geoms", " Capítulo 4 Visualización de datos 4.1 Paquetes necesarios para este capítulo Para este capítulo necesitas tener instalado el paquete tidyverse. En este capítulo se explicará qué es el paquete ggplot2 (Wickham 2016) y cómo utilizarlo para visualizar datos. Dado que este libro es un apoyo para el curso BIO4022, esta clase puede también ser seguida en este link. El video de la clase se encontrará disponible en este link 4.2 El esqueleto El esqueleto de una visualización usando ggplot2 es la siguiente ggplot(data.frame, aes(nombres_de_columna)) + geom_algo(argumentos, aes(columnas)) + theme_algo() Como ejemplo para discutir usaremos el siguiente código que genera la figura 4.1: library(tidyverse) data(&quot;diamonds&quot;) ggplot(diamonds, aes(x = carat, y = price)) + geom_point(aes(color = cut)) + theme_classic() Figura 4.1: Gráfico en el cual gráficamos los quilates de diamantes versus su precio, con el corte del diamante representado por el color En este caso general, lo primero que ponemos después de ggplot es el data.frame desde el cual graficaremos algo. En el ejemplo de la figura 4.1 usamos la base de datos diamonds del paquete ggplot2 (Wickham 2016), luego dentro de aes ponemos las columnas que graficaremos como x y/o y. En nuestro ejemplo dentro de aes ponemos como eje x los quilates de los diamantes (caret) y como y el precio de los mismos (price). Ojo que existe la necesidad de poner aes en ggplot2 (algo que no había sido necesario cuando usamos dplyr o tidyr). 4.3 Por que usamos aes() y + Al ser el primer paquete creado en el tidyverse, ggplot2 tiene un par de convenciones distintas. Por un lado, cada vez que usamos el nombre de una columna que está en un data frame debemos usarlo dentro de la función aes. Además, cuando se creó el paquete ggplot2 no existia el pipeline (%&gt;%), por lo que se utilizaba el signo + con la misma función. 4.4 geom_algo Luego de especificar una base de datos, se debe continuar con un geom_algo, esto nos indicará que tipo de gráfico usaremos. Los gráficos pueden ser combinados como veremos en ejemplos futuros. 4.4.1 Una variable categórica una continua Primero veremos algunos de los geom que podemos utilizar con una variable categórica y una continua 4.4.1.1 geom_boxplot En la figura 4.2, generado a partir del código a continuación con la base de datos iris presente en R (Anderson 1935). data(&quot;iris&quot;) ggplot(iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot() Figura 4.2: Boxplot que representa los largos del sépalo de tres especies del género Iris Los boxplots muestran una línea gruesa central (la mediana), una caja, que delimita el primer y tercer cuartil y los bigotes, los cuales se extienden hasta los valores extremos. En el caso que estos valores estén por sobre 1.5 veces la distancia entre el primer y tercer cuartil, estos serán representados por puntos (siendo considerados outlyers). En la figura 4.2, sólo Iris virginica presenta un outlayer en cuanto a las medidas del largo del sépalo. Los boxplots, como todos los gráficos pueden ser personalizados usando otros argumentos, los que mostraremos en esta sección los iremos introduciendo de a poco. Si quisieramos por ejemplo que el color de las cajas del boxplot fueran dea cuerdo a la especie, cambiamos el llenado (fill) de la caja, como vemos en el siguiente ejemplo y figura 4.3 ggplot(iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot(aes(fill = Species)) Figura 4.3: Boxplot que representa los largos del sépalo de tres especies del género Iris, en este caso el color de la caja representa la especie Dos cosas a notar en este ejemplo, por un lado la leyenda se genera de forma automática, y por otro lado, vemos que es necesario poner Species dentro de aes, esto es debido a que Species es una columna y como se explicó al principio de este capítulo, todas las columnas deben ser incuidas dentro de la función aes para poder ser referenciadas. 4.4.1.2 geom_jitter Utilizando la misma base de datos, podemos crear un gráfico del tipo jitter. En este caso hay un punto por cada observación, lo cual puede ayudar a entender mejor los datos que tenemos. ggplot(iris, aes(x = Species, y = Sepal.Length)) + geom_jitter(aes(color = Species)) Figura 4.4: jitter plot que representa los largos del sépalo de tres especies del género Iris, en este caso el color de los puntos representan la especie En la figura 4.4 vemos los mismos datos que en la figura 4.2, el agregar el color = Species dentro del aes nos permite que el color de cada punto este determinado por la especie a la que pertenece. 4.4.1.3 Otros geom categóricos Otros geom categóricos que podemos explorar con esta base de datos son: geom_violin geom_bar geom_col 4.5 Combinando geoms Uno puede escribir varios geoms para formar un gráfico combinado. Por ejemplo, podríamos generar un gráfico con un boxplot y un jitter plot, como vemos en la figura 4.5 ggplot(iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot() + geom_jitter(aes(color = Species)) Figura 4.5: Boxplot y jitter plot combinados que representa los largos del sépalo de tres especies del género Iris. 4.5.1 El orden importa Si bien se pueden combinar los geom, el orden de estos importa, ya que ggplot2 genera las figuras por capas. Esto es ilustrado en la figura 4.6, en la cual al crear primero el jitter y luego el boxplot, las cajas del boxplot tapan los puntos, a diferencia de la figura 4.5 ggplot(iris, aes(x = Species, y = Sepal.Length)) + geom_jitter(aes(color = Species)) + geom_boxplot() Figura 4.6: Boxplot y jitter plot combinados que representa los largos del sépalo de tres especies del género Iris, en este caso al llamar al jitter antes del boxplot, las cajas tapan los puntos. 4.5.2 Dos variables continuas Algunos de los geoms que podemos usar para dos variables continuas son: geom_point geom_smooth geom_line geom_hex geom_rug Ahora veremos algunos de ellos: 4.5.2.1 geom_point Este geom es el que nos permite hacer un gráfico de dispersión en R. Para esto tenemos que poner variables continuas en x e y en ggplot y agregar la función geom_point, como vemos en el siguiente código y en la figura 4.7. data(&quot;ChickWeight&quot;) ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point() Figura 4.7: Gráfico en el cual vemos el peso de pollos en el tiempo Si quisieramos que el color de cada punto estuviera separado por dieta, podríamos agregarle aes(color = Diet) a geom_point. Sin embargo, deberíamos transformar Diet en factor, ya sea antes de usar ggplot o dentro de ggplot tal como vemos en el siguiente código y en la figura 4.8. data(&quot;ChickWeight&quot;) ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = factor(Diet))) Figura 4.8: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores distintos según el tipo de dieta 4.5.2.2 geom_smooth y stat_smooth 4.5.2.2.1 geom_smooth Estas funciones nos permiten generar líneas de tendencias con intervalos de confianza. Así si quisieramos ver líneas de tendencias para nuestro scatterplot, dependiendo de la dieta, usaríamos el siguiente código, el cual nos da la figura 4.9. ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = factor(Diet))) + geom_smooth(aes(fill = factor(Diet))) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figura 4.9: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores distintos según el tipo de dieta, con líneas de tendencia e intervalos de confianza basados en el método loess Por defecto, la función geom_smooth generará una tendencia basada en loess, lo cual es una correlación local. En general, es mejor hacer una línea de tendencia basado en modelos que uno puede explicar mejor como un modelo lineal. Para esto, cambiamos el argumento method a lm como en el siguiente código y la figura 4.10. ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = factor(Diet))) + geom_smooth(aes(fill = factor(Diet)), method = &quot;lm&quot;) Figura 4.10: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores distintos según el tipo de dieta, con líneas de tendencia e intervalos de confianza basados en modelos lineales 4.5.2.2.2 stat_smooth La función stat_smooth es más flexible que geom_smooth. La gran diferencia es que nos permite incluir una fórmula para expresar la relación entre \\(x\\) e \\(y\\). Por ejemplo, si pensaramos que en el caso de la base de datos ChickWeight la relación entre el peso y el tiempo se expresa mejor con una ecuación cuadrática (ver ecuación (4.1)) tendríamos el siguiente código que genera la figura 4.11. ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = factor(Diet))) + stat_smooth(aes(fill = factor(Diet)), method = &quot;lm&quot;, formula = y ~ x + I(x^2)) Figura 4.11: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores distintos según el tipo de dieta, con líneas de tendencia e intervalos de confianza basados en modelos lineales con una relación cuadrática \\[\\begin{equation} y = \\beta_2 x^2 + \\beta_1 x + c \\tag{4.1} \\end{equation}\\] 4.5.3 Combinando varios gráficos con facet_wrap Algunas veces, en particular si tenemos muchas variables categóricas, no es recomendable generar una línea o punto de color distinto para cada variable. Por ejemplo, si seguimos con el crecimiento de los pollos de la base de datos ChickWeight, vemos que la variable Chick representa cada pollo. Dado que hay varios pollos por dieta se vuelve confuso y poco informativo como se ve en la figura 4.12 generada con el siguiente código. ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = Diet)) + geom_line(aes(color = Diet, group = Chick)) Figura 4.12: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores distintos según el tipo de dieta y con líneas para cada pollo individual. Para aclarar este enredo, es mejor el generar un gráfico para cada dieta, y es ahí donde entra la función facet_wrap. Esta función nos permite generar el gráfico deseado al agregar como argumento dentro de la función el simbolo ~ seguido del nombre de la variable a utilizar para separar los gráficos, tal como en la figura 4.13 y su código correspondiente. ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = Diet)) + geom_line(aes(color = Diet, group = Chick)) + facet_wrap(~Diet) Figura 4.13: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores y gráficos distintos según el tipo de dieta y con líneas para cada pollo individual. Esta función siempre tendrá los mismos ejes y escala para todos los gráficos. Además, intentará siempre dejar la disposición de los gráficos de la forma más cuadrada posible, pero esto puede ser modificado agregando el argumento ncol y un número de columnas, así como vemos en la figura 4.14 y su código correspondiente. ggplot(ChickWeight, aes(x = Time, y = weight)) + geom_point(aes(color = Diet)) + geom_line(aes(color = Diet, group = Chick)) + facet_wrap(~Diet, ncol = 3) Figura 4.14: Gráfico en el cual vemos el peso de pollos en el tiempo, con colores y gráficos distintos según el tipo de dieta y con líneas para cada pollo individual. Referencias "],
["modelos.html", "Capítulo 5 Modelos en R 5.1 Paquetes necesarios para este capítulo 5.2 Modelos estadísticos", " Capítulo 5 Modelos en R 5.1 Paquetes necesarios para este capítulo Para este capítulo necesitas tener instalado el paquete tidyverse, broom y MuMIn. En este capítulo se explicará como generar modelos en R, el como obtener información y tablas a partir de los modelos con el paquete Broom (Robinson and Hayes 2018) y una leve introducción a la selección de modelos con el paquete MuMIn (Barton 2018) Dado que este libro es un apoyo para el curso BIO4022, esta clase puede también ser seguida en este link. El video de la clase se encontrará disponible en este [link]. 5.2 Modelos estadísticos Un modelo estadístico intenta explicar las causas de un suceso basado en un muestreo de la población total. El supuesto es que si la muestra que obtenemos de la población es representativa de esta, podremos inferir las causas de la variación de la población midiendo variables explicativas. En general tenemos una variable respuesta (fenómeno que queremos explicar), y una o varias variables explicativas que generarían deterministamente parte de la variabilidad en la variable respuesta. 5.2.1 Ejemplo Tomemos el ejemplo de la base de datos CO2 presente en R (Potvin, Lechowicz, and Tardif 1990). Supongamos que nos interesa saber que factores afectan la captación de \\(CO_2\\) en las plantas. En la tabla ?? vemos las primeras 20 observaciones de esta base de datos. Vemos que dentro de los factores que tenemos para explicar la captación de \\(CO_2\\) estan: Type: Subespecie de la planta (Missisipi o Quebec) Treatment: Tratamiento de la plnata, enfriado (chilled) o no enfriado (nonchilled) conc: Concentración ambiental de \\(CO_2\\), en mL/L. Una posible explicación que nos permitiría intentar explicar este fenómeno, es que las plantas de distintas subespecies, tendrán distinta captación de \\(CO_2\\), lo cual exlploramos en el gráfico 5.1: Figura 5.1: Captación de CO2 por plantas dependiente de su subespecie Vemos que se observa una tendencia a que las plantas con origen en Quebec capten más \\(CO_2\\) que las que estan en el Mississippi, pero ¿Podemos decir efectivamente que ambas poblaciónes tienen medias distintas medias? Es ahí donde entran los modelos. 5.2.2 Representando un modelo en R En R la mayoría de los modelos se representan con el siguiente codigo: alguna_funcion(Y ~ X1 + X2 + ... + Xn, data = data.frame) En este modelo, tenemos la variable respuesta Y, la cual puede estar explcada por una o multiples variables explicativas X, es por esto que el simbolo ~ se lee explicado por, donde lo que esta a su izquerada es la variable respuesta y a la derecha la variable explicativa. Los datos se encuentran en un data frame y finalmente usaremos alguna función, que identificará algún modelo. Algunas de estas funciones las encontramos en la tabla ?? 5.2.3 Volvamos al ejemplo de las plantas Para este ejemplo usaremos un modelo lineal simple, para esto siguiendo la tabla ?? usaremos la función lm: Fit1 &lt;- lm(uptake ~ Type, data = CO2) 5.2.3.1 usando broom para sacarle más a tu modelo El paquete broom (Robinson and Hayes 2018) es un paquete adyacente al tidyverse (por lo que debes cargarlo aparte del tidyverse), el cual nos permite tomar información de modelos generados en formato tidy. Hoy veremos 3 funciones de broom, estas son glance, tidy y augment. 5.2.3.1.1 glance la función glance, nos entregará información general del modelo, como el valor de p, el \\(R^2\\), log-likelihood, grados de libertad, y/o otros parametros dependiendo del modelo a utilizar. Esta información nos es entregada en un formato de data frame, como vemos en el código siguiente y en la tabla ?? library(broom) glance(Fit1) 5.2.3.1.2 tidy la función tidy, nos entregará información sobre los parametros del modelo, esto es el intercepto, la pendiente y/o interacciones, como vemos en el código siguiente y en la tabla ?? tidy(Fit1) 5.2.3.1.3 augment la función augment, nos entregará para cada observación de nuestro modelo, varios parametros importantes como el valor predicho, los residuales, el distancia de cook entre otros, esto nos sirve principalmente para estudiar los supuestos de nuestro modelo. A continuación vemos el uso de la función augment y 20 de sus observaciones en la tabla ?? augment(Fit1) 5.2.3.2 Selección de modelos usando broom y el AIC El AIC, o Criterio de informacion de Akaike (Aho, Derryberry, and Peterson 2014), es una medida de cuanta información nos entrega un modelo dada su complejidad. Esta última medida a partir del número de parámetros que tiene. Cuanto más bajo sea el AIC, mejor comparativamente es un modelo, y en general, un modelo que sea dos unidades de AIC menor que otro modelo, será considerado un modelo que es significativamente mejor que otro. La formula del criterio de selección de Akaike es la que vemos en la ecuación (5.1). \\[\\begin{equation} AIC = 2 K - 2 \\ln{(\\hat{L})} \\tag{5.1} \\end{equation}\\] Donde \\(K\\) es el número de parametros, lo cual podemos ver con tidy, si vemos la tabla ??, vemos que el modelo Fit1 tiene 2 parametros, esto es \\(K\\) es igual a 2. El log-likelihood del modelo (\\(\\ln{(\\hat{L})}\\)) es el ajuste que este tiene a los datos. Cuanto más positivo es este valor mejor se ajusta el modelo a los datos, y cuanto mas negativo es, menos se ajusta a los datos, en nuestro modelo, usando glance, podemos ver que el valor del log-likelyhood del modelo es de -300.8 (ver tabla ??). Por lo tanto remplazando la ecuación (5.1), obtenemos 605.6, que es un valor muy cercano a los 608, que aparecen en el glance del modelo (tabla ??). 5.2.3.2.1 Modelos candidatos Veamos la figura 5.2. para pensar cuales podrían ser modelos interesantes a explorar. ggplot(CO2, aes(x = conc, y = uptake)) + geom_point(aes(color = Type, shape = Treatment), size = 3) Figura 5.2: Gráfico exploratorio para generar modelos de la base de datos CO2 Referencias "],
["loops.html", "Capítulo 6 Loops (purrr) y bibliografía (rticles) 6.1 Paquetes necesarios para este capítulo 6.2 Generando una receta 6.3 Empezando el loop", " Capítulo 6 Loops (purrr) y bibliografía (rticles) 6.1 Paquetes necesarios para este capítulo Para este capítulo necesitas tener instalado el paquete tidyverse. Probablemente uno de los puntos que marca la diferencia entre ser un usuario de un lenguaje de programación y un alguién que realmente programa. Es el momento en que una persona aprende a hacer loops. Los loops son una acción repetitiva en la cual una misma acción es realizada por el computador ahorrandonos mucho tiempo de escribir código y muchas veces tiempo de computación tambien. Existen varias formas de como realizar loops en R, los for loops, la familia de los apply y más recientemente el uso del paquete purrr (Henry and Wickham 2018) presente en el tidyverse. En este capítulo nos enfocaremos principalmente en el uso de este paquete, pero también explicaremos levemente el caso de los for loops. Dado que este libro es un apoyo para el curso BIO4022, esta clase puede también ser seguida en este link. El video de la clase se encontrará disponible en este [link]. 6.2 Generando una receta Como hacer un loop, es una repetición de un código multiples veces, generalmente lo que más nos combiene es generar la receta tomando en cuenta el primer elemento y luego repetirlo en un loop. 6.2.1 Dioxido de nitrógeno en Madrid Supongamos que queremos estudiar la concentración de dióxido de Nitrógeno en madrid en distintas estaciones, la base de datos puede ser encontrada en el siguiente link. Dentro de esta base de datos tenemos una carpeta con la calidad de aire de estaciones en Madrid, con un archivo para cada año. Supongamos que se quiere hacer lo siguiente, limitandose a las estaciones de Cuatro Caminos, El Pardo, Escuelas Aguirre, Moratalaz y Tres Olivos, calcular los promedios de \\(NO_2\\) para cada mes y cada año en estas estaciones. 6.2.1.1 Generando la receta Esto lo podemos hacer con un loop, pero antes generemos la receta tomando en cuenta solo el 2017. Para esto hacemos lo siguiente: Tomemos la base de datos de calidad de aire de Madrid Leeamos el año 2017 Limitemonos a las estaciones de Cuatro Caminos, El Pardo, Escuelas Aguirre, Moratalaz y Tres Olivos Agreguemos una columna con el año y una con el mes Calculemos los promedios de \\(NO_2\\) para cada mes Eliminemos las columnas innecesarias para estudiar el efecto del \\(NO_2\\) en Madrid Vamos paso a paso 6.2.1.1.1 leyendo la base de datos El primer paso es leer la base de datos, para esto usamos el tidyverse y cargamos además lubridate por si tenemos que trabajar con las fechas. En la tabla ?? vemos los resultados del código a continuación. library(tidyverse) library(lubridate) Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) 6.2.1.1.2 Limitemonos a las estaciones seleccionadas Revisando el archivo stations.csv, podemos ver que el código de estaciones que estudiaremos son 28079036, 28079008,28079058, 28079060 y 28079038, por lo que lo ponemos en un filter. El resultado de esto lo podemos ver en la tabla ?? Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) %&gt;% filter(station %in% c(28079036, 28079008, 28079058, 28079060, 28079038)) 6.2.1.1.3 Agreguemos aparte el mes, el año y el nombre de la estación Usando mutate y las funciones monthy year de lubridate podemos agregar el més y el año para cada observación, además usando left_joint, podemos agreagar el nombre de las estaciones usando la base de datos stations.csv. El resultado de esto lo podemos ver en la tabla ?? stations &lt;- read_csv(&quot;stations.csv&quot;) %&gt;% rename(station = id) Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) %&gt;% filter(station %in% c(28079036, 28079008, 28079058, 28079060, 28079038)) %&gt;% mutate(month = month(date), year = year(date)) %&gt;% left_join(stations) Finalmente, agrupamos sacamos el promedio por mes y sacamos las columnas sobrantes al mismo tiempo, como vemos en la tabla ?? library(lubridate) stations &lt;- read_csv(&quot;stations.csv&quot;) %&gt;% rename(station = id) ## Parsed with column specification: ## cols( ## id = col_integer(), ## name = col_character(), ## address = col_character(), ## lon = col_double(), ## lat = col_double(), ## elevation = col_integer() ## ) Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) %&gt;% filter(station %in% c(28079036, 28079008, 28079058, 28079060, 28079038)) %&gt;% mutate(month = month(date), year = year(date)) %&gt;% left_join(stations) %&gt;% group_by(month, name, year) %&gt;% summarise(NO_2 = mean(NO_2, na.rm = TRUE)) ## Parsed with column specification: ## cols( ## date = col_datetime(format = &quot;&quot;), ## BEN = col_double(), ## CH4 = col_character(), ## CO = col_double(), ## EBE = col_double(), ## NMHC = col_double(), ## NO = col_double(), ## NO_2 = col_double(), ## NOx = col_character(), ## O_3 = col_double(), ## PM10 = col_double(), ## PM25 = col_double(), ## SO_2 = col_double(), ## TCH = col_double(), ## TOL = col_double(), ## station = col_integer() ## ) ## Joining, by = &quot;station&quot; 6.2.1.1.4 Últimos detalles Vemos que hay algunos valores del 2018, esto parece raro, ya que leimos los archivos del 2017. Al revisar mas con summarize, vemos que en realidad son tan solo unas pocas observaciones las que generan esta anomalía debido a algunas medidas del 1 de enero del 2018. Para eliminarlas agregamos el siguiente código. library(lubridate) stations &lt;- read_csv(&quot;stations.csv&quot;) %&gt;% rename(station = id) ## Parsed with column specification: ## cols( ## id = col_integer(), ## name = col_character(), ## address = col_character(), ## lon = col_double(), ## lat = col_double(), ## elevation = col_integer() ## ) Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) %&gt;% filter(station %in% c(28079036, 28079008, 28079058, 28079060, 28079038)) %&gt;% mutate(month = month(date), year = year(date)) %&gt;% left_join(stations) %&gt;% group_by(month, name, year) %&gt;% summarise(NO_2 = mean(NO_2, na.rm = TRUE), n = n()) %&gt;% filter(n &gt; 500) ## Parsed with column specification: ## cols( ## date = col_datetime(format = &quot;&quot;), ## BEN = col_double(), ## CH4 = col_character(), ## CO = col_double(), ## EBE = col_double(), ## NMHC = col_double(), ## NO = col_double(), ## NO_2 = col_double(), ## NOx = col_character(), ## O_3 = col_double(), ## PM10 = col_double(), ## PM25 = col_double(), ## SO_2 = col_double(), ## TCH = col_double(), ## TOL = col_double(), ## station = col_integer() ## ) ## Joining, by = &quot;station&quot; Esto nos dá al fin la receta final que usaremos en el loop. 6.3 Empezando el loop En este capíulo usaremos principalmente la función map del paquete purrr para generar loops, en esta función los dos argumentos generales que necesitamos es un vector o lista (argumento .x) de los elementos que pasarán por una función, y una funcion (argumento .f) que se aplicará a toda esta lista. Es importante establecer que el resultado de map siempre será una lista. 6.3.1 Volvamos a nuestra receta Veamos el código que usamos para el año 2017 library(lubridate) stations &lt;- read_csv(&quot;stations.csv&quot;) %&gt;% rename(station = id) Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) %&gt;% filter(station %in% c(28079036, 28079008, 28079058, 28079060, 28079038)) %&gt;% mutate(month = month(date), year = year(date)) %&gt;% left_join(stations) %&gt;% group_by(month, name, year) %&gt;% summarise(NO_2 = mean(NO_2, na.rm = TRUE), n = n()) %&gt;% filter(n &gt; 500) La primera parte del código es la lectura del archivo Madrid2017 &lt;- read_csv(&quot;csvs_per_year/madrid_2017.csv&quot;) Para hacer esto por todos los archivos de la base de datos requeriríamos de una lista o vector con los nombres de cada uno de los archivos. ¡Si solo hubiera una función en R que nos permitiera leer los archivos de una carpeta! La función list.files hace eso. Entonces el código que vemos abajo genera un vector con todos los nombres de los archivos que queremos incorporar: Archivos &lt;- list.files(&quot;csvs_per_year&quot;, full.names = TRUE) Archivos ## [1] &quot;csvs_per_year/madrid_2001.csv&quot; &quot;csvs_per_year/madrid_2002.csv&quot; ## [3] &quot;csvs_per_year/madrid_2003.csv&quot; &quot;csvs_per_year/madrid_2004.csv&quot; ## [5] &quot;csvs_per_year/madrid_2005.csv&quot; &quot;csvs_per_year/madrid_2006.csv&quot; ## [7] &quot;csvs_per_year/madrid_2007.csv&quot; &quot;csvs_per_year/madrid_2008.csv&quot; ## [9] &quot;csvs_per_year/madrid_2009.csv&quot; &quot;csvs_per_year/madrid_2010.csv&quot; ## [11] &quot;csvs_per_year/madrid_2011.csv&quot; &quot;csvs_per_year/madrid_2012.csv&quot; ## [13] &quot;csvs_per_year/madrid_2013.csv&quot; &quot;csvs_per_year/madrid_2014.csv&quot; ## [15] &quot;csvs_per_year/madrid_2015.csv&quot; &quot;csvs_per_year/madrid_2016.csv&quot; ## [17] &quot;csvs_per_year/madrid_2017.csv&quot; &quot;csvs_per_year/madrid_2018.csv&quot; Entonces poner dentro de map, un vector con el nombre de los archivos (Archivos), y una función para leer los archivos (read_csv). Esto es el siguiente código Madrid &lt;- map(Archivos, read_csv) Genera una lista donde cada elemento es un data frame de un año de mediciones. Cuando se agregan otras funciones mas complejas en un loop usando map. Como por ejemplo filter, ponemos un simbolo ~ dentro de map, y un .x dentro de filter para representar a cada dataframe que usaremos. Madrid &lt;- map(Archivos, read_csv) %&gt;% map(~filter(.x, station %in% c(28079036, 28079008, 28079058, 28079060, 28079038))) De esta forma podemos seguir la receta creada anteriormente sin ningún problema. Madrid &lt;- map(Archivos, read_csv) %&gt;% map(~filter(.x, station %in% c(28079036, 28079008, 28079058, 28079060, 28079038))) %&gt;% map(~mutate(.x, month = month(date), year = year(date))) %&gt;% map(~left_join(.x, stations)) %&gt;% map(~group_by(.x, month, name, year)) %&gt;% map(~summarise(.x, NO_2 = mean(NO_2, na.rm = TRUE), n = n())) %&gt;% map(~filter(.x, n &gt; 500)) Pero en este momento tenemos una lista con 17 data frames, en vez de un gran data frame con todos los datos. Para esto debenos unir esta lista usando la función reduce, lo cual nos genera el siguiente código y la tabla ?? library(lubridate) Madrid &lt;- map(Archivos, read_csv) %&gt;% map(~filter(.x, station %in% c(28079036, 28079008, 28079058, 28079060, 28079038))) %&gt;% map(~mutate(.x, month = month(date), year = year(date))) %&gt;% map(~left_join(.x, stations)) %&gt;% map(~group_by(.x, month, name, year)) %&gt;% map(~summarise(.x, NO_2 = mean(NO_2, na.rm = TRUE), n = n())) %&gt;% map(~filter(.x, n &gt; 500)) %&gt;% reduce(bind_rows) Referencias "],
["presentacion.html", "Capítulo 7 Presentaciones en R 7.1 Paquetes necesarios para este capítulo", " Capítulo 7 Presentaciones en R 7.1 Paquetes necesarios para este capítulo Para este capítulo necesitas tener instalado el paquete rmarkdown. En este capítulo aprenderemos dos formas distintas de hacer presentaciones en R Dado que este libro es un apoyo para el curso BIO4022, esta clase puede también ser seguida en este link. El video de la clase se encontrará disponible en este link. "],
["soluciones.html", "Capítulo 8 Soluciones a problemas 8.1 Capítulo 1 8.2 Capítulo 2 8.3 Capítulo 3", " Capítulo 8 Soluciones a problemas Todos los problemas en programación tienen más de una forma de llegar a ellos, es por esto que las soluciones acá mostradas deben tomarse solo como una referencia, y revisar si el resultado final de tu código (aunque sea distinto de este), sea igual al que presentamos. 8.1 Capítulo 1 8.1.1 Ejercicio 1 Algunas posibles soluciones: storms %&gt;% filter(status == &quot;hurricane&quot;) %&gt;% select(year, wind, hu_diameter) %&gt;% group_by(year) %&gt;% summarize_all(mean) storms %&gt;% filter(status == &quot;hurricane&quot; &amp; !is.na(hu_diameter)) %&gt;% select(year, wind, hu_diameter) %&gt;% group_by(year) %&gt;% summarize_all(mean) storms %&gt;% filter(status == &quot;hurricane&quot;) %&gt;% select(year, wind, hu_diameter) %&gt;% group_by(year) %&gt;% summarize_all(funs(mean), na.rm = TRUE) 8.1.2 Ejercicio 2 Una de las soluciones posibles: Solution &lt;- mpg %&gt;% filter(year &gt; 2004 &amp; class == &quot;compact&quot;) %&gt;% mutate(kpl = (cty * 1.609)/3.78541) 8.2 Capítulo 2 8.2.1 Ejercicio 1 Una posible solución a este problema sería: `r mean((iris %&gt;% filter(Species == &quot;virginica&quot;))$Petal.Length)` 8.3 Capítulo 3 8.3.1 Ejercicio 1 8.3.1.1 a Sola &lt;- Huemul %&gt;% dplyr::select(lon, lat, basisOfRecord) %&gt;% filter(!is.na(lat) &amp; !is.na(lon)) 8.3.1.2 b Solb &lt;- Huemul %&gt;% group_by(basisOfRecord) %&gt;% summarize(N = n()) 8.3.2 Ejercicio 2 8.3.2.1 a Primero bajamos la base de datos, lo cual se puede hacer de forma manual o como en el código siguiente utilizando la función download.file download.file(&quot;http://www.ine.cl/docs/default-source/medioambiente-(micrositio)/variables-b%C3%A1sicas-ambientales-(vba)/aire/dimensi%C3%B3n-aire-factor-estado.xlsx?sfvrsn=4&quot;, destfile = &quot;test.xlsx&quot;) Una vez bajada esta base de datos utilizaremos los paquetes readxl para leer los archivos excel, tidyverse para manipular los datos y stringr para trabajar con texto. library(readxl) library(tidyverse) library(stringr) Partimos leyendo la pestaña que contiene las estaciones meteorológicas con su código: EM &lt;- read_excel(&quot;test.xlsx&quot;, sheet = &quot;T001&quot;) Luego para poder más adelante unir esta base de datos con otras, cambiamos el nombre de la columna Codigo_Est_Meteoro a Est_Meteoro como aparece en las otras bases de datos. EM &lt;- EM %&gt;% rename(Est_Meteoro = Codigo_Est_Meteoro) Luego empezamos a trabajar con la base de datos de temperatura media, para esto leemos la pestaña E10000003 TempMedia &lt;- read_excel(&quot;test.xlsx&quot;, sheet = &quot;E10000003&quot;) Existen varias variables que no utilizaremos, por ejemplo el código de la variable, y la unidad de medida. Además vemos que la variable día, siempre tiene valor 0, por lo cuál podemos eliminarla también. TempMedia &lt;- TempMedia %&gt;% select(-Codigo_variable, -Unidad_medida, -Día) Además podemos cambiar los nombres de la columna ValorF que no tiene ningún significado a TempMedia y Año a Year, esta última variable es cambiada solo por que la letra Ñ puede no ser leída por todos los computadores. TempMedia &lt;- TempMedia %&gt;% rename(TempMedia = ValorF, Year = Año) Si nos fijamos, hay algunos años, en los cuales todos los meses aparecen como 13, esto nos indica que en estos años no se registró en que mes se realizó la medición, por lo cual se eliminarán esas obsevaciones. TempMedia &lt;- TempMedia %&gt;% filter(Mes != 13) Posterior a esto, unumos la base de datos TempMedia con la base de datos EM y seleccionamos tan solo las columnas que nos interesan y finalmente transformamos el mes en una variable numérica: ## Joining, by = &quot;Est_Meteoro&quot; TempMedia &lt;- left_join(TempMedia, EM) %&gt;% select(Mes, Year, TempMedia, Ciudad_localidad) %&gt;% mutate(Mes = as.numeric(Mes)) Si hicieramos todo esto en un comando tendriamos el siguiente código TempMedia &lt;- read_excel(&quot;test.xlsx&quot;, sheet = &quot;E10000003&quot;) %&gt;% select(-Codigo_variable, -Unidad_medida, -Día) %&gt;% rename(TempMedia = ValorF, Year = Año) %&gt;% filter(Mes != 13) %&gt;% left_join(EM) %&gt;% select(Mes, Year, TempMedia, Ciudad_localidad) %&gt;% mutate(Mes = as.numeric(Mes)) De la misma manera modificamos el código de arriba para la humedad con la salvedad que la columna de día no tiene tilde en esta pestaña a la fecha de 19 de Agosto del 2018: ## Joining, by = &quot;Est_Meteoro&quot; HumMedia &lt;- read_excel(&quot;test.xlsx&quot;, sheet = &quot;E10000006&quot;) %&gt;% dplyr::select(-Codigo_variable, -Unidad_medida, -Dia) %&gt;% rename(HumMedia = ValorF, Year = Año) %&gt;% filter(Mes != 13) %&gt;% left_join(EM) %&gt;% dplyr::select(Mes, Year, HumMedia, Ciudad_localidad) %&gt;% mutate(Mes = as.numeric(Mes)) En el siguiente código unimos las dos bases de datos, vemos las primeras 20 observaciones de la base de datos resultante en la tabla ?? TempHum &lt;- full_join(TempMedia, HumMedia) ## Joining, by = c(&quot;Mes&quot;, &quot;Year&quot;, &quot;Ciudad_localidad&quot;) Con esto vemos que la humedad media no es medida en los mismos años ni en todos los lugares que se mide la temperatura media, pero como ambas variables nos interesan por igual, la mantenemos de todas maneras con sus valores NA 8.3.2.2 b El segundo ejercicio es mucho mas simple, donde solo tenemos que agrupar por ciudad y mes, y usar summarize_all para las funciones mean y sd como vemos en la tabla ?? TempHumMonthly &lt;- TempHum %&gt;% select(-Year) %&gt;% group_by(Mes, Ciudad_localidad) %&gt;% summarize_all(funs(mean, sd), na.rm = TRUE) "],
["referencias.html", "Capítulo 9 Referencias", " Capítulo 9 Referencias "]
]
