<style>
.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}
</style>

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

Clase 5 Modelos
========================================================
author: Derek Corcoran
date: "`r format(Sys.time(), '%d/%m, %Y')`"
autosize: true
transition: rotate

<STYLE TYPE="text/css">
<!--
  td{
    font-family: Arial; 
    font-size: 4pt;
    padding:0px;
    cellpadding="0";
    cellspacing="0"
  }
  th {
    font-family: Arial; 
    font-size: 4pt;
    height: 20px;
    font-weight: bold;
    text-align: right;
    background-color: #ccccff;
  }
  table { 
    border-spacing: 0px;
    border-collapse: collapse;
  }
--->
</STYLE>


¿Qué es un modelo?
=================
incremental:true


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
suppressMessages(suppressWarnings(library(tidyverse, quietly = TRUE)))
```


* Un modelo es una versión simplificada de la realidad que nos permite hacer inferencias o prediccións sobre una población
* Un modelo es un resumen adecuado de la realidad
* Un modelo es una simplificación or aproximación a la realidad y por ende no reflejará toda la realidad (Burnham y Anderson)
* Todos los modelos estan equivocados, algunos son útiles (George Box)

Veamos un ejemplo
================

* ¿Cuánto $CO_2$ captan las plantas?

```{r, echo = FALSE, cache = TRUE}
data("CO2")
knitr::kable((CO2 %>% sample_n(20)), row.names = FALSE)
```

¿Será la subespecie?
================

```{r}
ggplot(CO2, aes(x = Type, y=uptake)) + geom_boxplot() + geom_jitter(aes(color = Type)) + theme_classic()
```

¿Como lo determinamos?

Formula de un modelo
=====================

```{r, echo = TRUE, eval=FALSE}
alguna_funcion(Y ~ X1 + X2 + ... + Xn, data = data.frame)
```

* **Y:** Variable respuesta (Captación de $CO_2$)
* **`~`:** Explicado por
* **$X_n$:** Variable explicativa n (Subespecie, tratamiento, etc.)
* **data.frame:*** Base de datos (CO2)
* **alguna_funcion:** El modelo a testear (nuestra simplificación de la realidad)

Algunos modelos en R
====================

```{r Modelos, echo = FALSE}
Modelos <- data.frame(Modelos = c("Prueba de t" ,"ANOVA", "Modelo lineal simple", "modelo lineal generalizado", "Modelo aditivo", "Modelo no lineal", "modelos lineales mixtos", "Boosted regression trees"), Funcion = c("t.test()", "aov()", "lm()", "glm()", "gam()", "nls()", "lmer()", "gbm()"))

knitr::kable(Modelos, row.names = FALSE)
```

¿Cual usamos para estudiar lo de la planta?
==============

```{r, echo = TRUE}
Fit1 <- lm(uptake ~ Type, data = CO2)
```

* por este ejercicio usaremos un modelo lineal simple

Usando broom para sacarle mas a tu modelo (glance)
==============

* Para ver datos generales del modelo

```{r, eval = FALSE}
library(broom)
glance(Fit1)
```

```{r, echo=FALSE}
library(broom)
knitr::kable(glance(Fit1))
```

Usando broom para sacarle mas a tu modelo (tidy)
==============

* Para ver parametros del modelo

```{r, eval = FALSE}
tidy(Fit1)
```

```{r, echo=FALSE}
knitr::kable(tidy(Fit1))
```


Usando broom para sacarle mas a tu modelo (augment)
==============

* Para ver predicciones y residuales del modelo

```{r, eval = FALSE}
augment(Fit1)
```

```{r, echo=FALSE}
knitr::kable(augment(Fit1))
```

Selección de modelos
=======================
class: small-code
incremental:true

* Basado en criterios de información
* Trabajaremos con AIC
* $K$ número de parámetros
* $\ln{(\hat{L})}$ ajuste, mas positivo mejor, mas negativo es malo

$$AIC = 2 K - 2 \ln{(\hat{L})}$$

Modelos candidatos
=======================
class: small-code
incremental:true

```{r, echo = FALSE}
ggplot(CO2, aes(x = conc, y = uptake)) + geom_point(aes(color = Type, shape = Treatment), size = 2)
```


Modelos candidatos
=======================
class: small-code
incremental:true

```{r, echo = TRUE}
Fit1 <- lm(uptake ~ Type, data = CO2)
Fit2 <- lm(uptake ~ Treatment, data = CO2)
Fit3 <- lm(uptake ~ conc, data = CO2)
Fit4 <- lm(uptake ~ Type + Treatment + conc, data = CO2)
```

Selección de modelos con broom
=======================
class: small-code
incremental:true

```{r, echo = TRUE}
Modelo1 <- glance(Fit1) %>% dplyr::select(r.squared, AIC) %>% mutate(Modelo = "Fit1")
Modelo2 <- glance(Fit2) %>% dplyr::select(r.squared, AIC) %>% mutate(Modelo = "Fit2")
Modelo3 <- glance(Fit3) %>% dplyr::select(r.squared, AIC) %>% mutate(Modelo = "Fit3")
Modelo4 <- glance(Fit3) %>% dplyr::select(r.squared, AIC) %>% mutate(Modelo = "Fit3")

Modelos <- bind_rows(Modelo1, Modelo2, Modelo3, Modelo4) %>% arrange(AIC) %>% mutate(DeltaAIC = AIC-min(AIC))
```

Selección de modelos con broom
=======================
class: small-code
incremental:true

```{r}
knitr::kable(Modelos)
```

Selección de modelos en una variable (forma)
=======
```{r}
TempHum <- readRDS("TempHum.rds")
PA <- TempHum %>% filter(Ciudad_localidad == "Punta Arenas")

```



GLMs
===========

Distribuciones
=========================

```{r}
knitr::include_graphics("Distributions.jpeg", dpi = 300)
```


Estructura de error
========================================================
incremental:true
* **family =**
* gaussian (variable dependiente continua)
* binomial (variable dependiente 0 o 1)
* poisson (variable dependiente cuentas 1, 2 ,3 ,4 ,5)
* gamma (variable dependiente continua solo positiva)


Modelo lineal generalizado (familia: binomial)
========================================================
class: small-code

```{r, results= "asis"}
library(tidyverse)
train <- read_csv("train.csv") %>% filter(Embarked == "S")
knitr::kable(head(train[,-c(1,4,7,8,9)]), format = "html")
```

Modelo lineal generalizado (familia: binomial)
========================================================
class: small-code

```{r}
FitBin <- glm(Survived ~ Fare + Sex, data = train)

DF <- expand.grid(Fare = seq(from = min(train$Fare), to = max(train$Fare), length.out = 50), Prediction = NA, SE = NA, Sex = c("male", "female"))


DF$Prediction <- predict(FitBin, DF, se.fit = TRUE)$fit
DF$SE <- predict(FitBin, DF, se.fit = TRUE)$se.fit

ggplot(DF, aes(x = Fare, y = Prediction)) + geom_ribbon(aes(ymax= Prediction + SE, ymin =Prediction - SE, fill = Sex), alpha = 0.5) + geom_line(aes(lty = Sex)) + theme_classic()

```


Modelo lineal generalizado (familia: binomial)
========================================================

```{r}
knitr::kable(broom::tidy(FitBin))
```

```{r}
knitr::kable(broom::glance(FitBin))
```

$R^2$: `r DescTools::PseudoR2(FitBin, "Nagelkerke")`


Modelo lineal generalizado (familia: binomial)
========================================================
class: small-code

```{r}
FitBin <- glm(Survived ~ Fare + Sex, data = train)

DF <- expand.grid(Fare = seq(from = -200, to = 1000, length.out = 50), Prediction = NA, SE = NA, Sex = c("male", "female"))


DF$Prediction <- predict(FitBin, DF, se.fit = TRUE)$fit
DF$SE <- predict(FitBin, DF, se.fit = TRUE)$se.fit

ggplot(DF, aes(x = Fare, y = Prediction)) + geom_ribbon(aes(ymax= Prediction + SE, ymin =Prediction - SE, fill = Sex), alpha = 0.5) + geom_line(aes(lty = Sex)) + theme_classic() + geom_hline(yintercept = c(0,1), lty = 2 , color = "red")

```




Modelo lineal generalizado (familia: binomial)
========================================================
class: small-code

```{r, results="asis", cache = TRUE}
FitBin2 <- glm(Survived ~ Fare*Sex, data = train, family = binomial())
##1
DF <- expand.grid(Fare = seq(from = min(train$Fare), to = max(train$Fare), length.out = 50), Prediction = NA, SE = NA, Sex = c("male", "female"))

DF$Prediction <- predict(FitBin2, DF, se.fit = TRUE, type = "response")$fit
DF$SE <- predict(FitBin2, DF, se.fit = TRUE,"response")$se.fit


ggplot(DF, aes(x = Fare, y = Prediction)) + geom_ribbon(aes(ymax= Prediction + SE, ymin =Prediction - SE, fill = Sex), alpha = 0.5) + geom_line(aes(lty = Sex)) + theme_classic()
```

Modelo lineal generalizado (familia: binomial)
========================================================

```{r}
knitr::kable(broom::tidy(FitBin2))
```

```{r}
knitr::kable(broom::glance(FitBin2))
```

$R^2$: `r DescTools::PseudoR2(FitBin2, "Nagelkerke")`

Modelo lineal generalizado (familia: binomial)
========================================================
class: small-code

```{r, results="asis", cache = TRUE}
FitBin2 <- glm(Survived ~ Fare*Sex, data = train, family = binomial())
##1
DF <- expand.grid(Fare = seq(from = -200, to = 1000, length.out = 50), Prediction = NA, SE = NA, Sex = c("male", "female"))

DF$Prediction <- predict(FitBin2, DF, se.fit = TRUE, type = "response")$fit
DF$SE <- predict(FitBin2, DF, se.fit = TRUE,"response")$se.fit


ggplot(DF, aes(x = Fare, y = Prediction)) + geom_ribbon(aes(ymax= Prediction + SE, ymin =Prediction - SE, fill = Sex), alpha = 0.5) + geom_line(aes(lty = Sex)) + theme_classic()
```

Función link
=========== 
incremental:true

* Actua sobre $Y$
* family Gaussian, link = identidad
* family Gamma, link = inverso
* family poisson, link  = log
* family binomial, link = logit

$$Logit = log{\frac{p}{1-p}}$$

Función link
=========== 

```{r}
x = c(-1, -0.8, 0.1, 0.2, 0.5, 0.8, 1, 2, 2.3)
link = data.frame(Valor = x, Identidad = x, Inverso = 1/x, Log = log(x), logit = log(x/(1-x)))
knitr::kable(link)
```

Función link
=========== 

```{r}
x = seq(from = -4, to = 4, by = 0.2)
link = data.frame(Valor = x, Identidad = x, Inverso = 1/x, Log = log(x), logit = log(x/(1-x)))
link2 <- link %>% gather(key = link, value = transformacion, -Valor)
ggplot(link2, aes(x = Valor, y = transformacion)) + geom_line(aes(color = link)) + geom_point(aes(color = link)) + theme_classic() + geom_vline(xintercept = c(0,1), lty = 2 , color = "red")
```

Ajuste
===========

Pseudo $R^2$

Expandiendo los modelos que puedo usar paquete caret
===========================================
incremental:true

* Paquete [caret](http://topepo.github.io/caret/index.html), 238 tipos de modelos distintos 
* Si quieren aprender mucho más acá hay un [tutorial](https://www.youtube.com/watch?v=7Jbb2ItbTC4&t=3s)
* En lo más básico solo una función *train*
* Curso de de machine learning en R?

función train
===================
class: small-code

* Sirve para cualquier modelo, solo hay que cambiar method
```{r}
library(caret)
Eficiencia <- train(mpg ~ wt, method = "lm", data = mtcars)
glance(Eficiencia$finalModel)
```
***
```{r}
library(caret)
Eficiencia2 <- train(mpg ~ wt, method = "glm", data = mtcars)
glance(Eficiencia2$finalModel)
```

función train
===================
class: small-code

```{r, cache = TRUE}
library(caret)
Eficiencia3 <- train(mpg ~ wt, method = "bagEarth", data = mtcars)
Eficiencia3$results
```

función train (clasificación)
===================
class: small-code

```{r, eval = FALSE}
library(caret)
Especies <- train(Species ~. , method = "glm", data = iris)
```

función train (clasificación)
===================
class: small-code

```{r}
library(caret)
Especies <- train(Species ~. , method = "rpart", data = iris)
Especies$results
```

función train (clasificación)
===================
class: small-code
```{r}
library(rattle)
library(rpart.plot)
rpart.plot(Especies$finalModel)
```

Para la próxima clase
=====================
incremental:true

* Loops normales y con purrr
* Plantillas de Journals para trabajar desde r (Instalar *rticles*)
* Hay que poder knitear a pdf instalar *tinytex*
